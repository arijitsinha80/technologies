## Google Cloud Dataflow :o:
## Google Cloud Dataflow :hand: 


|          |                           |
| -------- | ------------------------- |
| title    | Google Cloud Dataflow     | 
| status   | 10                        |
| section  | Workflow-Orchestration    |
| keywords | Workflow-Orchestration    |


    
Google Cloud Dataflow is a unified programming model and a managed
service for developing and executing a wide variety of data processing
patterns (pipelines). Dataflow includes SDKs for defining data
processing workflows and a Cloud platform managed services to run
those workflows on a Google cloud platform resources such as Compute
Engine, BigQuery amongst others [@www-Dataflow]. Dataflow
pipelines can operate in both batch and streaming mode. The platform
resources are provided on demand, allowing users to scale to meet
their requirements, it's also optimized to help balance lagging work
dynamically.

Being a cloud offering, Dataflow is designed to allow users to focus
on devising proper analysis without worrying about the installation
and maintaining the underlying data piping and process
infrastructure [@www-GoogleLiveStream].

### Duplicated entry: merge

     
Google Cloud DataFlow is a unified programming model that manages the
deployment, maintenance and optimization of data processes such as
batch processing, ETL etc [@www-cloud-google1]. It creates a
pipeline of tasks and dynamically allocates resources thereby
maintaining high efficiency and low latency. These capabilities make
it suitable for solving challenging big data problems
[@www-cloud-google1]. Also, google DataFlow overcomes the
performance issues faced by Hadoops Mapreduce while building
pipelines [@www-dataconomy].  The performance of MapReduce started
deteriorating while facing multiple petabytes of data whereas Google
Cloud Dataflow is apparently better at handling enormous datasets
[@www-cloud-google1]. Additionally Google Dataflow can be
integrated with Cloud Storage, Cloud Pub/Sub, Cloud Datastore, Cloud
Bigtable, and BigQuery. The unified programming ability is another
noteworthy feature which uses Apache Beam SDKs to support powerful
operations like windowing and allows correctness control to be applied
to batch and stream data processes.




    
